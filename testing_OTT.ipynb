{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9064bb-0d5f-462a-9a99-5a9191692d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "import pickle\n",
    "from tqdm.auto import trange, tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_from_disk\n",
    "from omegaconf import OmegaConf\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "from citylearn.agents.rbc import HourRBC\n",
    "from citylearn.agents.q_learning import TabularQLearning\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.reward_function import RewardFunction\n",
    "from citylearn.wrappers import NormalizedObservationWrapper\n",
    "from citylearn.wrappers import StableBaselines3Wrapper\n",
    "from citylearn.wrappers import TabularQLearningWrapper\n",
    "\n",
    "from stable_baselines3.a2c import A2C\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from trajectory.models.gpt import GPT, GPTTrainer\n",
    "\n",
    "from trajectory.utils.common import pad_along_axis\n",
    "from trajectory.utils.discretization import KBinsDiscretizer\n",
    "from trajectory.utils.env import create_env\n",
    "import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49838ec3-4e1b-4eb1-95a1-92a7514d066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_trajectory(trajectories):\n",
    "    states, traj_lens, returns = [], [], []\n",
    "    for i in trajectories:\n",
    "        states.append(trajectories[i][\"states\"])\n",
    "        traj_lens.append(len(trajectories[i][\"states\"]))\n",
    "        returns.append(np.array(trajectories[i][\"rewards\"]).sum())\n",
    "    num_timesteps = sum(traj_lens)\n",
    "    sorted_inds = np.argsort(returns)  # lowest to highest\n",
    "    num_trajectories = 1\n",
    "    timesteps = traj_lens[sorted_inds[-1]]\n",
    "    ind = len(trajectories) - 2\n",
    "    while ind >= 0 and timesteps + traj_lens[sorted_inds[ind]] < num_timesteps:\n",
    "        timesteps += traj_lens[sorted_inds[ind]]\n",
    "        num_trajectories += 1\n",
    "        ind -= 1\n",
    "    sorted_inds = sorted_inds[-num_trajectories:]\n",
    "    print(sorted_inds)\n",
    "            #print(trajectories[1])\n",
    "    for ii in sorted_inds:\n",
    "        print(ii)\n",
    "            #print(trajectories[0].keys())\n",
    "    trajectories = [trajectories[int(ii)] for ii in sorted_inds]\n",
    "    for trajectory in trajectories:\n",
    "        for key in trajectory.keys():\n",
    "            trajectory[key] = np.array(trajectory[key])\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da70d576-f65b-4a58-b129-e7de00fd1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_trajectory(states, actions, rewards, discount=0.99):\n",
    "    traj_length = states.shape[0]\n",
    "    # I can vectorize this for all dataset as once,\n",
    "    # but better to be safe and do it once and slow and right (and cache it)\n",
    "    \n",
    "    if actions.ndim == 3 :\n",
    "        actions = actions.reshape(actions.shape[0],actions.shape[1])\n",
    "    \n",
    "    if rewards.ndim == 1 :\n",
    "        rewards = rewards.reshape(rewards.shape[0],1)\n",
    "        \n",
    "    print(\"Discount \"+str(discount))\n",
    "    discounts = (discount ** np.arange(traj_length))\n",
    "\n",
    "    values = np.zeros_like(rewards)\n",
    "    for t in range(traj_length):\n",
    "        # discounted return-to-go from state s_t:\n",
    "        # r_{t+1} + y * r_{t+2} + y^2 * r_{t+3} + ...\n",
    "        # .T as rewards of shape [len, 1], see https://github.com/Howuhh/faster-trajectory-transformer/issues/9\n",
    "        values[t] = (rewards[t + 1:].T * discounts[:-t - 1]).sum()\n",
    "    print(states.shape)\n",
    "    print(actions.shape)\n",
    "    print(rewards.shape)\n",
    "    print(values.shape)\n",
    "\n",
    "    joined_transition = np.concatenate([states, actions, rewards, values], axis=-1)\n",
    "\n",
    "    return joined_transition\n",
    "\n",
    "def segment(states, actions, rewards, terminals):\n",
    "    assert len(states) == len(terminals)\n",
    "    \n",
    "    trajectories = {}\n",
    "\n",
    "    episode_num = 0\n",
    "    for t in trange(len(terminals), desc=\"Segmenting\"):\n",
    "        if episode_num not in trajectories:\n",
    "            trajectories[episode_num] = {\n",
    "                \"states\": [],\n",
    "                \"actions\": [],\n",
    "                \"rewards\": []\n",
    "            }\n",
    "        \n",
    "        trajectories[episode_num][\"states\"].append(states[t])\n",
    "        trajectories[episode_num][\"actions\"].append(actions[t])\n",
    "        trajectories[episode_num][\"rewards\"].append(rewards[t])\n",
    "\n",
    "        if terminals[t]:\n",
    "            # next episode\n",
    "            episode_num = episode_num + 1\n",
    "\n",
    "    trajectories_lens = [len(v[\"states\"]) for k, v in trajectories.items()]\n",
    "\n",
    "    for t in trajectories:\n",
    "        trajectories[t][\"states\"] = np.stack(trajectories[t][\"states\"], axis=0)\n",
    "        trajectories[t][\"actions\"] = np.stack(trajectories[t][\"actions\"], axis=0)\n",
    "        trajectories[t][\"rewards\"] = np.stack(trajectories[t][\"rewards\"], axis=0)\n",
    "\n",
    "    return trajectories, trajectories_lens\n",
    "\n",
    "class DiscretizedDataset(Dataset):\n",
    "    def __init__(self, trajectories,traj_lengths, env_name=\"city_learn\", num_bins=100, seq_len=10, discount=0.99, strategy=\"uniform\", cache_path=\"data\"):\n",
    "        self.seq_len = seq_len\n",
    "        self.discount = discount\n",
    "        self.num_bins = num_bins\n",
    "        self.dataset = dataset\n",
    "        self.env_name = env_name\n",
    "        \n",
    "        #trajectories, traj_lengths = segment(self.dataset[\"observations\"],self.dataset[\"actions\"],self.dataset[\"rewards\"],self.dataset[\"dones\"])\n",
    "        #trajectories = self.sort_trajectory(trajectories)\n",
    "        self.trajectories = trajectories\n",
    "        self.traj_lengths = traj_lengths\n",
    "        self.cache_path = cache_path\n",
    "        self.cache_name = f\"{env_name}_{num_bins}_{seq_len}_{strategy}_{discount}\"\n",
    "        \n",
    "        self.joined_transitions = []\n",
    "        for i,t in enumerate(tqdm(trajectories, desc=\"Joining transitions\")):\n",
    "            self.joined_transitions.append(\n",
    "                    join_trajectory(trajectories[i][\"states\"], trajectories[i][\"actions\"], trajectories[i][\"rewards\"],discount = self.discount)\n",
    "                )\n",
    "        \"\"\"\n",
    "        if cache_path is None or not os.path.exists(os.path.join(cache_path, self.cache_name)):\n",
    "            self.joined_transitions = []\n",
    "            for t in tqdm(trajectories, desc=\"Joining transitions\"):\n",
    "                self.joined_transitions.append(\n",
    "                    join_trajectory(trajectories[t][\"states\"], trajectories[t][\"actions\"], trajectories[t][\"rewards\"],discount = self.discount)\n",
    "                )\n",
    "\n",
    "            os.makedirs(os.path.join(cache_path), exist_ok=True)\n",
    "            # save cached version\n",
    "            with open(os.path.join(cache_path, self.cache_name), \"wb\") as f:\n",
    "                pickle.dump(self.joined_transitions, f)\n",
    "        else:\n",
    "            with open(os.path.join(cache_path, self.cache_name), \"rb\") as f:\n",
    "                self.joined_transitions = pickle.load(f)\n",
    "        \"\"\"\n",
    "\n",
    "        self.discretizer = KBinsDiscretizer(\n",
    "            np.concatenate(self.joined_transitions, axis=0),\n",
    "            num_bins=num_bins,\n",
    "            strategy=strategy\n",
    "        )\n",
    "\n",
    "        # get valid indices for seq_len sampling\n",
    "        indices = []\n",
    "        for path_ind, length in enumerate(traj_lengths):\n",
    "            end = length - 1\n",
    "            for i in range(end):\n",
    "                indices.append((path_ind, i, i + self.seq_len))\n",
    "        self.indices = np.array(indices)\n",
    "\n",
    "    def get_env_name(self):\n",
    "        return self.env.name\n",
    "\n",
    "   \n",
    "\n",
    "    def get_discretizer(self):\n",
    "        return self.discretizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(idx)\n",
    "        traj_idx, start_idx, end_idx = self.indices[idx]\n",
    "        \n",
    "        joined = self.joined_transitions[traj_idx][start_idx:end_idx]\n",
    "        \n",
    "\n",
    "        loss_pad_mask = np.ones((self.seq_len, joined.shape[-1]))\n",
    "        if joined.shape[0] < self.seq_len:\n",
    "            # pad to seq_len if at the end of trajectory, mask for padding\n",
    "            loss_pad_mask[joined.shape[0]:] = 0\n",
    "            joined = pad_along_axis(joined, pad_to=self.seq_len, axis=0)\n",
    "\n",
    "        joined_discrete = self.discretizer.encode(joined).reshape(-1).astype(np.longlong)\n",
    "        loss_pad_mask = loss_pad_mask.reshape(-1)\n",
    "\n",
    "        return joined_discrete[:-1], joined_discrete[1:], loss_pad_mask[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be94b5a7-68d6-4c74-b886-fba42a1d9a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa45051a-4654-42bd-a18b-80b556402a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config_path = \"configs/medium/city_learn_traj.yaml\",offline_data_path = None,device = \"cpu\"):\n",
    "    config = OmegaConf.load(config_path)\n",
    "\n",
    "    if offline_data_path is None :\n",
    "        offline_data_path = config.trainer.offline_data_path\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:1\"\n",
    "\n",
    "  \n",
    "    wandb.init(\n",
    "            **config.wandb,\n",
    "            config=dict(OmegaConf.to_container(config, resolve=True))\n",
    "        )\n",
    "    \n",
    "    offline_data_path = offline_data_path\n",
    "    dataset = load_from_disk(offline_data_path)\n",
    "\n",
    "    trajectories, traj_lengths = segment(dataset[\"observations\"],dataset[\"actions\"],dataset[\"rewards\"],dataset[\"dones\"])\n",
    "    offline_trajectories = sort_trajectory(trajectories)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(5, offline_trajectories)\n",
    "\n",
    "\n",
    "    datasets = DiscretizedDataset(offline_trajectories,traj_lengths,discount = config.dataset.discount, seq_len = config.dataset.seq_len, strategy = config.dataset.strategy)\n",
    "    dataloader = DataLoader(datasets,  batch_size=config.dataset.batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    trainer_conf = config.trainer\n",
    "    data_conf = config.dataset\n",
    "\n",
    "    model = GPT(**config.model)\n",
    "    model.to(device)\n",
    "    \n",
    "\n",
    "    num_epochs = config.trainer.num_epochs\n",
    "\n",
    "    warmup_tokens = len(datasets) * data_conf.seq_len * config.model.transition_dim\n",
    "    final_tokens = warmup_tokens * num_epochs\n",
    "\n",
    "    trainer = GPTTrainer(\n",
    "            final_tokens=final_tokens,\n",
    "            warmup_tokens=warmup_tokens,\n",
    "            action_weight=trainer_conf.action_weight,\n",
    "            value_weight=trainer_conf.value_weight,\n",
    "            reward_weight=trainer_conf.reward_weight,\n",
    "            learning_rate=trainer_conf.lr,\n",
    "            betas=trainer_conf.betas,\n",
    "            weight_decay=trainer_conf.weight_decay,\n",
    "            clip_grad=trainer_conf.clip_grad,\n",
    "            eval_seed=trainer_conf.eval_seed,\n",
    "            eval_every=trainer_conf.eval_every,\n",
    "            eval_episodes=trainer_conf.eval_episodes,\n",
    "            eval_temperature=trainer_conf.eval_temperature,\n",
    "            eval_discount=trainer_conf.eval_discount,\n",
    "            eval_plan_every=trainer_conf.eval_plan_every,\n",
    "            eval_beam_width=trainer_conf.eval_beam_width,\n",
    "            eval_beam_steps=trainer_conf.eval_beam_steps,\n",
    "            eval_beam_context=trainer_conf.eval_beam_context,\n",
    "            eval_sample_expand=trainer_conf.eval_sample_expand,\n",
    "            eval_k_obs=trainer_conf.eval_k_obs,  # as in original implementation\n",
    "            eval_k_reward=trainer_conf.eval_k_reward,\n",
    "            eval_k_act=trainer_conf.eval_k_act,\n",
    "            checkpoints_path=trainer_conf.checkpoints_path,\n",
    "            save_every=1,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    trainer.train(\n",
    "        model=model,\n",
    "        dataloader=dataloader,\n",
    "        num_epochs=num_epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54b2163-cbd6-43ee-b8c8-833881783e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-stud15/anaconda3/envs/stable3/lib/python3.9/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory data_interactions/sac_dataset.pkl not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigs/medium/city_learn_ott.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config_path, offline_data_path, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39mwandb,\n\u001b[1;32m     12\u001b[0m         config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(OmegaConf\u001b[38;5;241m.\u001b[39mto_container(config, resolve\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     15\u001b[0m offline_data_path \u001b[38;5;241m=\u001b[39m offline_data_path\n\u001b[0;32m---> 16\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffline_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m trajectories, traj_lengths \u001b[38;5;241m=\u001b[39m segment(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m\"\u001b[39m],dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m\"\u001b[39m],dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m\"\u001b[39m],dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdones\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     19\u001b[0m offline_trajectories \u001b[38;5;241m=\u001b[39m sort_trajectory(trajectories)\n",
      "File \u001b[0;32m~/anaconda3/envs/stable3/lib/python3.9/site-packages/datasets/load.py:2689\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2687\u001b[0m fs, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m url_to_fs(dataset_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m   2688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m-> 2689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n\u001b[1;32m   2691\u001b[0m     posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   2692\u001b[0m ):\n\u001b[1;32m   2693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory data_interactions/sac_dataset.pkl not found"
     ]
    }
   ],
   "source": [
    "train(config_path = \"configs/medium/city_learn_ott.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dfd97d-741a-4467-8f67-bdb5fd5c7160",
   "metadata": {},
   "source": [
    "## Online Tuning Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd66bf08-5e80-4b2c-b057-3f0fe4db12b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"configs/medium/city_learn_traj_v1_seq20_rf_CombinedReward_norm_wrapper.yaml\"\n",
    "config = OmegaConf.load(config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1105528c-21d3-4cba-afde-c808988b775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(**config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "996e41a5-a38f-4b10-a838-d1a23047ef88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tok_emb): Embedding(5100, 128)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): EinLinear(n_models=51, in_features=128, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61ef6326-a7e2-486f-8e95-fd806beb650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(param.numel() for param in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "683cbffb-9e07-476c-99b8-bce5230c1bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2229504"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436df9e-c163-4beb-b34b-adc68c676609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
